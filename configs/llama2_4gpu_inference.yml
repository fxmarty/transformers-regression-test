defaults:
  - base_config # inherits from base config
  - _self_ # for hydra 1.1 compatibility

experiment_name: llama_4gpu_inference

model: daryl149/llama-2-7b-chat-hf
device: cuda

benchmark:
  memory: true
  duration: 20
  new_tokens: 200
  input_shapes:
    sequence_length: 200

backend:
  device_map: auto

hydra:
  sweeper:
    params:
      benchmark.input_shapes.batch_size: 1,2,4,16
      backend.torch_dtype: float16,float32

